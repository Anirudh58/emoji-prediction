{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc5a19-7120-4590-b799-5906bfcd7f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from src import emojilib\n",
    "\n",
    "# nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p\n",
    "import emoji\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd336163-9d0b-4d32-b4cb-541a72d714aa",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d59b5-fe06-413a-84b3-55231c171f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path variables\n",
    "root_path = './../'\n",
    "raw_tweets_file = 'tweet_by_ID_30_4_2022__08_11_07.txt'\n",
    "clean_data_folder_path = os.path.join(root_path, 'data', 'clean_data')\n",
    "raw_data_folder_path = os.path.join(root_path, 'data', 'raw_data')\n",
    "raw_tweets_path = os.path.join(raw_data_folder_path, raw_tweets_file)\n",
    "\n",
    "# target emojis\n",
    "mapping = { \n",
    "    '‚ù§':'0' , 'üòç':'1' , 'üòÇ':'2' , 'üíï':'3' , \n",
    "    'üî•':'4' , 'üòä':'5' , 'üòé':'6' , '‚ú®':'7' , \n",
    "    'üíô':'8' , 'üòò':'9' , 'üì∑':'10' , 'üá∫üá∏':'11' , \n",
    "    '‚òÄ':'12' , 'üíú':'13' , 'üòâ':'14' , 'üíØ':'15' , \n",
    "    'üòÅ':'16' , 'üéÑ':'17' , 'üì∏':'18' , 'üòú':'19'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b04ed-32c3-4b14-9168-3531a1451059",
   "metadata": {},
   "source": [
    "## Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047bdc65-e39d-4e00-b7b5-597ec2c4870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress consecutive occurences of each emoji to one. eg: \"Hilarious üòÇüòÇüòÇ\" becomes \"Hilarious üòÇ\"\n",
    "def compress_tweet(tweet):\n",
    "    \n",
    "    # get emoji list\n",
    "    emoji_list = emojilib.emoji_list(tweet)\n",
    "    \n",
    "    emoji_done = set()\n",
    "    for emoji in emoji_list:\n",
    "        emoji_code = emoji['code']\n",
    "        if emoji_code not in emoji_done:\n",
    "            # replace \"<emoji><emoji><emoji>\" as \"<emoji>\"\n",
    "            tweet = re.sub(f'{emoji_code}+', f' {emoji_code} ', tweet)\n",
    "            \n",
    "            # replace \"<emoji> <emoji> <emoji>\" as \"<emoji>\"\n",
    "            tweet = re.sub(f'{emoji_code} +', f' {emoji_code} ', tweet)\n",
    "            \n",
    "            emoji_done.add(emoji_code)\n",
    "            \n",
    "    # remove extra space\n",
    "    tweet = re.sub(f' +', f' ', tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d2ad7-ba6c-4ded-89bb-e6866f861e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweet(text):\n",
    "    \n",
    "    # clean everything except emoji\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, \n",
    "                  p.OPT.HASHTAG, p.OPT.RESERVED,\n",
    "                  p.OPT.NUMBER, p.OPT.SMILEY)\n",
    "    \n",
    "    cleaned_tweet = p.clean(text)\n",
    "    \n",
    "    # remove all punctuations\n",
    "    cleaned_tweet = \"\".join(l for l in cleaned_tweet if l not in string.punctuation)\n",
    "    \n",
    "    # remove this weird special character (seems to be occurring in many tweets)\n",
    "    cleaned_tweet = re.sub('[‚Ä¶]', '', cleaned_tweet)\n",
    "    \n",
    "    # compress tweet - compress multiple consecutive emojis to one\n",
    "    cleaned_tweet = compress_tweet(cleaned_tweet)\n",
    "    \n",
    "    # get the list of emojis\n",
    "    emoji_list = emojilib.emoji_list(cleaned_tweet)\n",
    "    \n",
    "    # tokenize emojis\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, \n",
    "                  p.OPT.HASHTAG, p.OPT.RESERVED,\n",
    "                  p.OPT.NUMBER, p.OPT.SMILEY, \n",
    "                  p.OPT.EMOJI)    \n",
    "        \n",
    "    # for some reason this emoji alone is not being tokenized\n",
    "    cleaned_tweet_1 = cleaned_tweet.replace('üá∫üá∏', '<EMOJI>')\n",
    "    \n",
    "    # replaces each emoji with a special token $EMOJI$\n",
    "    tokenized_tweet = p.tokenize(cleaned_tweet_1)\n",
    "    tokenized_tweet = tokenized_tweet.replace('$EMOJI$', '<EMOJI>')\n",
    "    \n",
    "    # returning the tweet with no emoji as well\n",
    "    untokenized_tweet = tokenized_tweet.replace('<EMOJI> ', '')\n",
    "\n",
    "    return emoji_list, cleaned_tweet, tokenized_tweet, untokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432bf0d-489e-4e1d-a6f4-ef5eea8bd9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check a sample parse tweet\n",
    "parse_tweet(\"üòÇüòÇüòÇ Oh my god... That was hilarious!!! I am not sleeping ‚Ä¶ tonight wow üòÇüòÇüòÇ ‚Ä¶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447181f-d462-43db-b850-a8ece75aba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweets(clean_data_folder_path, raw_tweets_file):\n",
    "    \n",
    "    # this file contains the raw tweets. Present inside data/raw_data/\n",
    "    raw_tweets_file_name = raw_tweets_file.split('/')[-1]\n",
    "    \n",
    "    # open file handles to the files where we dump\n",
    "    out_text = open(os.path.join(clean_data_folder_path, \"tweets.text\"), 'w')\n",
    "    out_labels = open(os.path.join(clean_data_folder_path, \"tweets.labels\"), 'w')\n",
    "    out_ids = open(os.path.join(clean_data_folder_path, \"tweets.ids\"), 'w')\n",
    "    out_tokenized = open(os.path.join(clean_data_folder_path, \"tweets.tokenized\"), 'w')\n",
    "    out_notoken = open(os.path.join(clean_data_folder_path, \"tweets.notoken\"), 'w')\n",
    "    \n",
    "    count = 0\n",
    "    with open(raw_tweets_file) as f_in:\n",
    "        for line in tqdm.tqdm(f_in):\n",
    "            \n",
    "            # each line is a json file with a lot of information. load the json\n",
    "            json_data = json.loads(line)\n",
    "            \n",
    "            # extract the tweet id\n",
    "            tweet_id = json_data['id']\n",
    "            \n",
    "            # extract the raw text\n",
    "            tweet_text = json_data['text'].replace(\"\\n\",\"\")\n",
    "            \n",
    "            # parse the tweet. check parse_tweet() function above for all details\n",
    "            emoji_list, cleaned_tweet, tokenized_tweet, untokenized_tweet = parse_tweet(tweet_text)\n",
    "            \n",
    "            # print(f\"emoji_list: {emoji_list}\")\n",
    "            # print(f\"cleaned_tweet: {cleaned_tweet}\")\n",
    "            # print(f\"tokenized_tweet: {tokenized_tweet}\")\n",
    "            # print(\"\\n\\n\")\n",
    "            \n",
    "            # we ignore all tweets that have 0 emojis or more than 1 emoji\n",
    "            if len(emoji_list) != 1:\n",
    "                continue\n",
    "            \n",
    "            # dump clean tweet\n",
    "            out_text.write(cleaned_tweet+\"\\n\")\n",
    "            \n",
    "            # dump tokenized tweet - tokenized tweet is basically clean tweet with <EMOJI> token \n",
    "            # instead of the actual emoji itself\n",
    "            out_tokenized.write(tokenized_tweet+\"\\n\")\n",
    "            \n",
    "            try:\n",
    "                # +1 to account for the start token\n",
    "                emoji_word_location = tokenized_tweet.split(' ').index('<EMOJI>') + 1\n",
    "            except Exception as e:\n",
    "                print(f\"tokenized_tweet: {tokenized_tweet}\")\n",
    "                print(f\"emoji_list: {emoji_list}\")\n",
    "                continue\n",
    "            \n",
    "            # untokenized_tweet is the clean tweet (<EMOJI> token is also removed)\n",
    "            out_notoken.write(untokenized_tweet+\"\\n\")\n",
    "            \n",
    "            # dump the tweet id (not sure if we need this)\n",
    "            out_ids.write(str(tweet_id)+\"\\n\")\n",
    "            \n",
    "            # dump the emoji data as space separated triplets (code, location, name)\n",
    "            # UPDATE: len(emoji_list) should be just 1\n",
    "            for emoji in emoji_list:\n",
    "                location = emoji['location']\n",
    "                code = emoji['code']\n",
    "                name = emoji['name']\n",
    "                out_labels.write(f\"{mapping[code]},{location[0]},{emoji_word_location},{name}\")\n",
    "            out_labels.write(\"\\n\") \n",
    "            \n",
    "            count += 1\n",
    "            # if count > 50:\n",
    "            #     break\n",
    "    print(f\"Total tweets saved: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf75979-85c1-412d-af88-42ee6cd51774",
   "metadata": {},
   "source": [
    "## Parse tweets (Run only once). Generated data available through the data.zip drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0eced5-988a-4bed-abaf-f0e12da54931",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_freq = parse_tweets(clean_data_folder_path, raw_tweets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06b98b-a31e-428e-98e2-9e827977cccc",
   "metadata": {},
   "source": [
    "## Visualize the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d83cf8-ae38-4191-8edb-88a662a2c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_freq = defaultdict(int)\n",
    "with open(os.path.join(clean_data_folder_path, 'tweets.labels'), 'r') as f:\n",
    "    for line in f:\n",
    "        emoji = line.rstrip()\n",
    "        emoji_code = int(emoji.split(',')[0])\n",
    "        emoji_freq[emoji_code] += 1\n",
    "plt.bar(range(len(emoji_freq)), list(emoji_freq.values()), align='center')\n",
    "plt.xticks(range(len(emoji_freq)), list(emoji_freq.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321dadc2-29aa-4a36-853f-4b14f6edf742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c2fa6-2dab-4ba9-948c-c1326f393515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_proj2",
   "language": "python",
   "name": "nlp_proj2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
