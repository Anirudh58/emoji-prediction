{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7c739-0a4a-4752-9580-9f4cd8c1df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "# data\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5cf2d1-d906-47b6-99b1-1483acbbd0de",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac0c4f-433f-45e7-b4f3-e88e476d10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './../'\n",
    "clean_data_folder_path = os.path.join(root_path, 'data', 'clean_data')\n",
    "glove_path = os.path.join(root_path, \"glove.840B.300d.conll_filtered.txt\")\n",
    "\n",
    "# target emojis\n",
    "mapping = { \n",
    "    '‚ù§':'0' , 'üòç':'1' , 'üòÇ':'2' , 'üíï':'3' , \n",
    "    'üî•':'4' , 'üòä':'5' , 'üòé':'6' , '‚ú®':'7' , \n",
    "    'üíô':'8' , 'üòò':'9' , 'üì∑':'10' , 'üá∫üá∏':'11' , \n",
    "    '‚òÄ':'12' , 'üíú':'13' , 'üòâ':'14' , 'üíØ':'15' , \n",
    "    'üòÅ':'16' , 'üéÑ':'17' , 'üì∏':'18' , 'üòú':'19'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7809e-cd19-45e6-badf-5e8a5c9e5b23",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bccc57-9213-4fa0-9999-f92da3d4f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, dataset_path, transforms=None):\n",
    "        tweet_text_path = os.path.join(dataset_path, 'tweets.text')\n",
    "        tweet_label_path = os.path.join(dataset_path, 'tweets.labels')\n",
    "        tweet_tokenized_path = os.path.join(dataset_path, 'tweets.tokenized')\n",
    "        \n",
    "        # init glove\n",
    "        self.glove_emb = self.read_GloVe(glove_path)\n",
    "        \n",
    "        self.word_sentences = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # curate the sentences\n",
    "        count = 0\n",
    "        for line in open(tweet_tokenized_path).readlines():\n",
    "            current_sentence = ['<START>']\n",
    "            current_sentence.extend(line.rstrip().split(' '))\n",
    "            current_sentence.append('<END>')\n",
    "            self.word_sentences.append(current_sentence)\n",
    "            \n",
    "            # count += 1\n",
    "            # if count > 50:\n",
    "            #     break\n",
    "                \n",
    "        # curate the labels\n",
    "        count = 0\n",
    "        for line in open(tweet_label_path).readlines():\n",
    "            emojis = line.rstrip().split(' ')\n",
    "            \n",
    "            try:\n",
    "                emoji_code = int(emojis[0].split(',')[0][1:])\n",
    "                self.labels.append(emoji_code)\n",
    "            except Exception as e:\n",
    "                # no emoji for this tweet\n",
    "                print(line)\n",
    "                self.labels.append(-1)\n",
    "            \n",
    "            # count += 1\n",
    "            # if count > 50:\n",
    "            #     break\n",
    "        \n",
    "        # compute char sentences from word sentences\n",
    "        self.char_sentences = self.sentences2char(self.word_sentences)\n",
    "        \n",
    "        # compute counts\n",
    "        self.word_counts = Counter([w for l in self.word_sentences for w in l])\n",
    "        self.char_counts = Counter([c for l in self.word_sentences for w in l for c in w])\n",
    "        self.singletons = set([w for (w,c) in self.word_counts.items() if c == 1 and not w in self.glove_emb.keys()])\n",
    "        self.char_singletons = set([w for (w,c) in self.char_counts.items() if c == 1])\n",
    "        \n",
    "        # Build dictionaries to map from words, characters to indices and vice versa.\n",
    "        # Save first two words in the vocabulary for padding and \"UNK\" token.\n",
    "        self.word2i = {w:i+2 for i,w in enumerate(set([w for l in self.word_sentences for w in l] + list(self.glove_emb.keys())))}\n",
    "        self.char2i = {w:i+2 for i,w in enumerate(set([c for l in self.char_sentences for w in l for c in w]))}\n",
    "        self.i2word = {i:w for w,i in self.word2i.items()}\n",
    "        self.i2char = {i:w for w,i in self.char2i.items()}\n",
    "        \n",
    "        # compute vocab size\n",
    "        self.vocab_size = max(self.word2i.values()) + 1\n",
    "        self.char_vocab_size = max(self.char2i.values()) + 1\n",
    "        \n",
    "        # emoji dictionaries.\n",
    "        self.emoji2i = {e:int(i) for e,i in mapping.items()}\n",
    "        self.i2emoji = {i:e for e,i in self.emoji2i.items()}\n",
    "    \n",
    "    def sentences2char(self, sentences):\n",
    "        return [[['start'] + [c for c in w] + ['end'] for w in l] for l in sentences]\n",
    "    \n",
    "    def read_GloVe(self, filename):\n",
    "        embeddings = {}\n",
    "        for line in open(filename).readlines():\n",
    "            #print(line)\n",
    "            fields = line.strip().split(\" \")\n",
    "            word = fields[0]\n",
    "            embeddings[word] = [float(x) for x in fields[1:]]\n",
    "        return embeddings\n",
    "    \n",
    "    #When training, randomly replace singletons with UNK tokens sometimes to simulate situation at test time.\n",
    "    def getDictionaryRandomUnk(self, w, dictionary, train=False):\n",
    "        if train and (w in self.singletons and random.random() > 0.5):\n",
    "            return 1\n",
    "        else:\n",
    "            return dictionary.get(w, 1)\n",
    "        \n",
    "    #Map a list of sentences from words to indices.\n",
    "    def sentences2indices(self, words, dictionary, train=False):\n",
    "        #1.0 => UNK\n",
    "        return [[self.getDictionaryRandomUnk(w,dictionary, train=train) for w in l] for l in words]\n",
    "    \n",
    "    #Map a list of sentences containing to indices (character indices)\n",
    "    def sentences2indicesChar(self, chars, dictionary):\n",
    "        #1.0 => UNK\n",
    "        return [[[dictionary.get(c,1) for c in w] for w in l] for l in chars]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3d2aa-06e8-4ae2-87ed-5e352a9c187d",
   "metadata": {},
   "source": [
    "## Test the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bbd36-210c-4bb0-a946-2fd9ccfaddd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = EmojiDataset(clean_data_folder_path)\n",
    "\n",
    "test_idx = 5\n",
    "print(dataset.word_sentences[test_idx])\n",
    "print(dataset.char_sentences[test_idx])\n",
    "print(dataset.i2emoji[dataset.labels[test_idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d0f5c-af7e-45c7-8d92-863f1fec2047",
   "metadata": {},
   "source": [
    "## Utility Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ffd90-8129-4a7f-b81c-bba7c13b9295",
   "metadata": {},
   "source": [
    "### Pad inputs to max sequence length (for batching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce133e-eb9a-4fc1-82ac-c4b32caac86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(X_list):\n",
    "    X_padded = torch.nn.utils.rnn.pad_sequence([torch.as_tensor(l) for l in X_list], batch_first=True).type(torch.LongTensor) # padding the sequences with 0\n",
    "    X_mask   = torch.nn.utils.rnn.pad_sequence([torch.as_tensor([1.0] * len(l)) for l in X_list], batch_first=True).type(torch.FloatTensor) # consisting of 0 and 1, 0 for padded positions, 1 for non-padded positions\n",
    "    return (X_padded, X_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3ea32-84cb-4d0d-ba9c-a8a66051e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum word length (for character representations)\n",
    "MAX_CLEN=32\n",
    "\n",
    "def prepare_input_char(X_list):\n",
    "    MAX_SLEN = max([len(l) for l in X_list])\n",
    "    X_padded  = [l + [[]]*(MAX_SLEN-len(l))  for l in X_list]\n",
    "    X_padded  = [[w[0:MAX_CLEN] for w in l] for l in X_padded]\n",
    "    X_padded  = [[w + [1]*(MAX_CLEN-len(w)) for w in l] for l in X_padded]\n",
    "    return torch.as_tensor(X_padded).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257e36f-528d-45a0-9446-150f43b3e1b9",
   "metadata": {},
   "source": [
    "### Pad outputs using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76cad8d-8106-4ebf-887b-a83ef3e7b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_output_onehot(Y_list, NUM_TAGS=max(dataset.emoji2i.values())+1):\n",
    "    Y_onehot = [torch.zeros(len(l), NUM_TAGS) for l in Y_list]\n",
    "    for i in range(len(Y_list)):\n",
    "        for j in range(len(Y_list[i])):\n",
    "            Y_onehot[i][j,Y_list[i][j]] = 1.0\n",
    "    Y_padded = torch.nn.utils.rnn.pad_sequence(Y_onehot, batch_first=True).type(torch.FloatTensor)\n",
    "    return Y_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a227a28a-808b-4dfb-ac1e-d447546b857e",
   "metadata": {},
   "source": [
    "## Define training set and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece1eb5-c477-4617-b75d-11c3487be717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indices\n",
    "X       = dataset.sentences2indices(dataset.word_sentences, dataset.word2i, train=True)\n",
    "X_char  = dataset.sentences2indicesChar(dataset.char_sentences, dataset.char2i)\n",
    "Y       = dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c0547-e4a3-45b9-904d-4a005920921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max slen:\", max([len(x) for x in X_char]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f3317-a0ae-4463-8a05-5685b09a7697",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_padded, X_mask) = prepare_input(X)\n",
    "X_padded_char      = prepare_input_char(X_char)\n",
    "#Y_onehot           = prepare_output_onehot(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1da96a-a6ff-43e6-8fd1-09f349a659fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_padded:\", X_padded.shape)\n",
    "print(\"X_mask:\", X_mask.shape)\n",
    "print(\"X_padded_char:\", X_padded_char.shape)\n",
    "print(\"Y shape:\", len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbde428-e017-4cd0-9bf3-bae80ae3a57d",
   "metadata": {},
   "source": [
    "# Start Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e5e07-134d-453a-981e-27e1d793fbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
